# Proposal-001: Global Schema ID Management Across Kafka Clusters

**Status:** Research Complete
**Date:** 2025-11-13
**Author:** Research Team

---

## Executive Summary

This proposal presents a solution for managing Schema IDs globally across multiple Kafka Cluster deployments to prevent schema-id clashes and optimize schema distribution across regions. The research confirms that **Confluent Schema Registry already supports custom Schema ID provision**, enabling a higher-order global management system to coordinate Schema IDs across distributed Schema Registries.

**Key Finding:** Schema Registry supports custom Schema IDs through IMPORT and FORWARD modes, which allows external systems to manage Schema ID allocation globally.

---

## Problem Statement

### Current Challenge

When operating Kafka clusters across multiple regions, we face:

1. **Schema ID Clashes**: Independent Schema Registries in different regions generate conflicting Schema IDs
2. **Schema Proliferation**: Each region accumulates all schemas, regardless of relevance
3. **Scale Limits**: The 20,000 schema limit becomes a bottleneck
4. **Inefficient Distribution**: Schemas are replicated to regions where they're not used

### Desired Solution

We need a system where:

- A **global coordinator** manages Schema ID allocation centrally
- Schema IDs are **unique across all regions** globally
- Schemas can be **selectively deployed** to relevant regions via CI/CD
- **Gaps in Schema IDs** are acceptable and expected in regional deployments
- Only schemas for **relevant workloads** exist in each region's Schema Registry

---

## Research Questions

### 1. How is the Schema ID Currently Provided?

**Research Question:** How are Schema IDs generated and assigned in Confluent Schema Registry?

**Answer:**

Schema IDs are generated by the `IncrementalIdGenerator` class, which:
- Maintains a concurrent map of maximum IDs per context
- Generates sequential IDs starting from 1
- Operates independently per context (tenant + namespace)
- Retries up to 5 times (configurable via `kafkastore.write.max.retries`) on collision

**Code Location:**
- `core/src/main/java/io/confluent/kafka/schemaregistry/id/IncrementalIdGenerator.java`
- `core/src/main/java/io/confluent/kafka/schemaregistry/storage/KafkaSchemaRegistry.java:529-546`

**Default ID Generation Logic:**
```java
// From KafkaSchemaRegistry.java:529-546
String qctx = QualifiedSubject.qualifiedContextFor(tenant(), subject);
int retries = 0;
while (retries++ < kafkaStoreMaxRetries) {
  int newId = idGenerator.id(schemaValue);
  // Verify id is not already in use
  if (lookupCache.schemaKeyById(newId, qctx) == null) {
    schema.setId(newId);
    schemaValue.setId(newId);
    break;
  }
}
```

---

### 2. Can We Provide a Custom Schema ID via REST API?

**Research Question:** Does the Schema Registry REST API support providing a custom Schema ID during schema registration?

**Answer:** ✅ **YES** - The REST API supports custom Schema IDs through the `id` field in the registration request.

**API Endpoint:** `POST /subjects/{subject}/versions`

**Request Payload:**
```json
{
  "schema": "...",
  "schemaType": "AVRO",
  "id": 12345,
  "references": []
}
```

**Code Evidence:**
- `client/src/main/java/io/confluent/kafka/schemaregistry/client/rest/entities/requests/RegisterSchemaRequest.java:42`
  - Line 42: `private Integer id;` - Custom ID field
  - Lines 88-97: Getter/setter for custom ID

**Registration Processing:**
- `core/src/main/java/io/confluent/kafka/schemaregistry/rest/resources/SubjectVersionsResource.java:444-446`
  - Logs: "Registering new schema: subject {}, version {}, **id {}**, type {}, schema size {}"
  - Custom ID is passed through the entire registration pipeline

---

### 3. What Are the Constraints for Custom Schema IDs?

**Research Question:** What validation rules and constraints apply when providing custom Schema IDs?

**Answer:** Custom Schema IDs are subject to strict validation:

#### Mode Requirement

**Custom IDs require IMPORT or FORWARD mode** to be enabled.

**Code Location:** `core/src/main/java/io/confluent/kafka/schemaregistry/storage/AbstractSchemaRegistry.java:338-350`

```java
if (schema.getId() >= 0) {
  if (!getModeInScope(subject).isImportOrForwardMode()) {
    throw new OperationNotPermittedException("Subject " + subject
            + " in context " + context + " is not in import mode");
  }
}
```

#### ID Validation Rules

**Code Location:** `core/src/main/java/io/confluent/kafka/schemaregistry/storage/AbstractSchemaRegistry.java:767-782`

```java
public void checkIfSchemaWithIdExist(int id, Schema schema) {
  SchemaKey existingKey = this.lookupCache.schemaKeyById(id, qctx);
  if (existingKey != null) {
    SchemaValue existingSchemaValue = this.lookupCache.get(existingKey);
    Schema existingSchema = toSchemaEntity(existingSchemaValue);
    // Compare schemas
    if (!existingSchema.equals(schemaCopy)) {
      throw new OperationNotPermittedException(
        String.format("Overwrite new schema with id %s in context...", id));
    }
  }
}
```

**Validation Rules:**
1. ✅ Custom ID must be non-negative (>= 0)
2. ✅ If ID exists, schema content must match exactly (idempotent registration)
3. ❌ Cannot use an existing ID for a different schema
4. ✅ IDs can have gaps (not required to be sequential)
5. ✅ Subject must be in IMPORT or FORWARD mode

---

### 4. How Do We Configure IMPORT Mode?

**Research Question:** How do we enable IMPORT mode to allow custom Schema IDs?

**Answer:** IMPORT mode can be configured at global or subject level via REST API.

#### Prerequisites

**Configuration File:** Set in Schema Registry properties before starting:
```properties
mode.mutability=true
```

#### Setting IMPORT Mode via REST API

**Endpoint:** `PUT /mode/{subject}`

**Global Level (all subjects):**
```bash
curl -X PUT \
  -H "Content-Type: application/json" \
  http://localhost:8081/mode \
  --data '{"mode": "IMPORT"}'
```

**Subject Level:**
```bash
curl -X PUT \
  -H "Content-Type: application/json" \
  http://localhost:8081/mode/my-subject \
  --data '{"mode": "IMPORT"}'
```

**Query Parameter:** Use `?force=true` to force mode change even if schemas exist

**Code Location:** `core/src/main/java/io/confluent/kafka/schemaregistry/rest/resources/ModeResource.java:81-150`

#### Available Modes

**Code Location:** `core/src/main/java/io/confluent/kafka/schemaregistry/storage/Mode.java`

```java
public enum Mode {
  READWRITE,    // Default mode, auto-generate IDs
  READONLY,     // No schema modifications allowed
  READONLY_OVERRIDE, // @Deprecated
  IMPORT,       // Allow custom IDs, skip compatibility checks
  FORWARD       // Global-level only, forward writes to primary
}
```

**Key Differences:**
- **IMPORT mode**: Allows custom IDs, skips compatibility validation, intended for importing schemas
- **FORWARD mode**: Only supported at global level, forwards requests to primary datacenter
- **READWRITE mode**: Standard mode with auto-generated IDs and compatibility checks

---

### 5. What Are the Multi-Datacenter Patterns?

**Research Question:** How does Schema Registry handle distributed deployments and schema replication?

**Answer:** Schema Registry supports several multi-datacenter architectures:

#### Architecture Patterns

**1. Leader-Based Architecture**
- Single leader elected via Kafka consumer groups
- Non-leaders forward registration requests to leader
- Ensures sequential ID assignment
- Strong consistency guarantees

**Code Location:**
- `core/src/main/java/io/confluent/kafka/schemaregistry/leaderelector/kafka/KafkaGroupLeaderElector.java`
- `core/src/main/java/io/confluent/kafka/schemaregistry/storage/KafkaSchemaRegistry.java:614-629`

**2. Active-Passive with IMPORT Mode**
- Source (active): READONLY or READWRITE mode
- Destination (passive): IMPORT mode
- Replicator continuously copies schemas with IDs preserved
- One-way replication pattern

**3. Federated with FORWARD Mode**
- Secondary datacenters set `leader.eligibility=false`
- Schema Registry forwards writes to primary datacenter
- Requires cross-datacenter network connectivity
- Global consistency maintained

**4. Context Isolation**
- Different contexts (tenants) have independent ID sequences
- Enables multi-tenancy within single Schema Registry cluster
- Per-context ID management

**Code Location:** `client/src/main/java/io/confluent/kafka/schemaregistry/utils/QualifiedSubject.java`

---

## Proposed Solution Architecture

### Global Schema ID Coordinator

```
┌─────────────────────────────────────┐
│   Global Schema ID Coordinator      │
│   (Centralized ID Management)       │
│                                     │
│   • Assigns unique IDs globally     │
│   • Tracks ID allocation per schema │
│   • Provides ID reservation API     │
│   • Maintains global registry       │
└─────────────┬───────────────────────┘
              │
              │ Schema + ID Assignment
              ▼
┌─────────────────────────────────────┐
│         CI/CD Pipeline              │
│                                     │
│  • Request ID from coordinator      │
│  • Deploy to relevant regions       │
│  • Register with custom ID          │
└─────────────┬───────────────────────┘
              │
              │ Regional Deployment
     ┌────────┴─────────┬─────────────┐
     ▼                  ▼             ▼
┌─────────┐      ┌─────────┐   ┌─────────┐
│ Region A│      │ Region B│   │ Region C│
│ SR (ID: │      │ SR (ID: │   │ SR (ID: │
│ 1,5,7)  │      │ 2,3,8)  │   │ 4,6,9)  │
│ IMPORT  │      │ IMPORT  │   │ IMPORT  │
└─────────┘      └─────────┘   └─────────┘
```

### Implementation Steps

#### Phase 1: Global ID Coordinator Service

**Responsibilities:**
- Maintain global schema registry (schema content → ID mapping)
- Expose REST API for ID allocation
- Ensure uniqueness across all regions
- Support ID reservation and batch allocation

**API Design:**
```
POST /coordinator/schema/register
{
  "schema": "...",
  "schemaType": "AVRO",
  "references": []
}
Response:
{
  "id": 12345,
  "existing": false
}

GET /coordinator/schema/{id}
Response:
{
  "id": 12345,
  "schema": "...",
  "deployedRegions": ["us-east", "eu-west"]
}
```

#### Phase 2: Configure Regional Schema Registries

**Configuration per region:**

1. **Enable Mode Mutability** (schema-registry.properties):
```properties
mode.mutability=true
```

2. **Set IMPORT Mode** (per region):
```bash
curl -X PUT \
  -H "Content-Type: application/json" \
  http://region-sr:8081/mode \
  --data '{"mode": "IMPORT"}'
```

#### Phase 3: CI/CD Integration

**Schema Deployment Workflow:**

```yaml
# Example GitHub Actions / GitLab CI workflow
deploy-schema:
  steps:
    - name: Request Global Schema ID
      run: |
        RESPONSE=$(curl -X POST http://coordinator/schema/register \
          -H "Content-Type: application/json" \
          --data @schema.json)
        SCHEMA_ID=$(echo $RESPONSE | jq -r '.id')
        echo "SCHEMA_ID=$SCHEMA_ID" >> $GITHUB_ENV

    - name: Deploy to Relevant Regions
      run: |
        for region in $TARGET_REGIONS; do
          curl -X POST http://${region}-sr:8081/subjects/${SUBJECT}/versions \
            -H "Content-Type: application/json" \
            --data "{
              \"schema\": \"$(cat schema.json)\",
              \"id\": ${SCHEMA_ID}
            }"
        done
```

**Target Region Determination:**
- Configuration file mapping workloads → regions
- Schema annotations indicating deployment targets
- Service mesh metadata

#### Phase 4: Monitoring and Validation

**Health Checks:**
- Verify ID uniqueness across regions
- Detect ID conflicts or duplicates
- Monitor schema propagation delays
- Alert on registration failures

**Validation Tools:**
```bash
# Cross-region ID conflict checker
./scripts/validate-schema-ids.sh --all-regions

# Schema ID gap analyzer
./scripts/analyze-id-gaps.sh --region us-east
```

---

## Benefits

### 1. Eliminates Schema ID Clashes
- Global coordinator ensures unique IDs across all regions
- No conflicts when data flows between regions
- Consistent schema evolution globally

### 2. Optimized Schema Distribution
- Deploy only relevant schemas per region
- Reduces registry size and memory footprint
- Faster schema lookups and reduced latency

### 3. Breaks 20,000 Schema Limit
- Each region only stores subset of schemas
- Global capacity = regions × 20,000
- Scalable to hundreds of thousands of schemas

### 4. Flexible Deployment
- CI/CD-driven schema management
- Declarative schema configuration
- Version control for schema deployments

### 5. Audit and Governance
- Central visibility into all schemas
- Track schema usage per region
- Compliance and data lineage support

---

## Considerations and Trade-offs

### Advantages
✅ Native support in Schema Registry (IMPORT mode)
✅ Gap tolerance in Schema IDs
✅ Idempotent registration (same schema + ID = success)
✅ Context isolation for multi-tenancy
✅ REST API support for automation

### Challenges
⚠️ **Requires Central Coordinator**: Single point of dependency for ID allocation
⚠️ **Operational Complexity**: Additional service to maintain and monitor
⚠️ **CI/CD Integration**: Requires changes to deployment pipelines
⚠️ **Mode Configuration**: Must set IMPORT mode before deployment
⚠️ **Compatibility Skipped**: IMPORT mode bypasses compatibility checks

### Mitigations
- **High Availability**: Deploy coordinator with redundancy and backups
- **Idempotency**: Coordinator must handle duplicate requests gracefully
- **Fallback**: Allow manual ID override in emergency scenarios
- **Compatibility**: Run compatibility checks in CI/CD before coordinator registration
- **Documentation**: Comprehensive runbooks and troubleshooting guides

---

## Alternative Approaches Considered

### Alternative 1: Schema Linking (Confluent Platform 7.0+)
**Description:** Confluent's native schema replication feature

**Pros:**
- Officially supported by Confluent
- Automated schema propagation
- Built-in conflict resolution

**Cons:**
- Replicates ALL schemas (no selective deployment)
- Doesn't solve 20,000 limit problem
- Requires Schema Linking licenses

**Verdict:** ❌ Doesn't meet selective deployment requirement

---

### Alternative 2: Namespace/Context Isolation
**Description:** Use different contexts per region

**Pros:**
- Native Schema Registry feature
- Independent ID spaces per context

**Cons:**
- Breaks schema portability across regions
- Data migration complexity
- Requires context-aware clients

**Verdict:** ❌ Too restrictive, limits flexibility

---

### Alternative 3: Reserved ID Ranges
**Description:** Allocate non-overlapping ID ranges per region

**Pros:**
- No central coordinator needed
- Simple static allocation

**Cons:**
- Wastes ID space
- Inflexible capacity planning
- Still hits 20,000 limit per region

**Verdict:** ❌ Doesn't solve scale problem

---

## Implementation Roadmap

### Phase 1: Proof of Concept (2 weeks)
- [ ] Develop minimal Global ID Coordinator
- [ ] Test IMPORT mode in dev environment
- [ ] Validate custom ID registration
- [ ] Measure performance impact

### Phase 2: Pilot (4 weeks)
- [ ] Deploy coordinator in staging
- [ ] Configure 2 regional Schema Registries in IMPORT mode
- [ ] Integrate with CI/CD pipeline
- [ ] Run parallel deployment (old + new)
- [ ] Validate data flow across regions

### Phase 3: Production Rollout (8 weeks)
- [ ] Production-ready coordinator with HA
- [ ] Gradual region migration (one by one)
- [ ] Monitoring and alerting setup
- [ ] Runbooks and documentation
- [ ] Training for operations team

### Phase 4: Optimization (Ongoing)
- [ ] Performance tuning
- [ ] Cost optimization
- [ ] Feature enhancements (batch allocation, etc.)
- [ ] Integration with governance tools

---

## Code References

### Key Files Analyzed

1. **Schema ID Generation:**
   - `core/src/main/java/io/confluent/kafka/schemaregistry/id/IdGenerator.java`
   - `core/src/main/java/io/confluent/kafka/schemaregistry/id/IncrementalIdGenerator.java`

2. **REST API:**
   - `client/src/main/java/io/confluent/kafka/schemaregistry/client/rest/entities/requests/RegisterSchemaRequest.java`
   - `core/src/main/java/io/confluent/kafka/schemaregistry/rest/resources/SubjectVersionsResource.java:433-510`

3. **Schema Registration Logic:**
   - `core/src/main/java/io/confluent/kafka/schemaregistry/storage/KafkaSchemaRegistry.java:412-561`
   - `core/src/main/java/io/confluent/kafka/schemaregistry/storage/AbstractSchemaRegistry.java:302-350,767-782`

4. **Mode Management:**
   - `core/src/main/java/io/confluent/kafka/schemaregistry/storage/Mode.java`
   - `core/src/main/java/io/confluent/kafka/schemaregistry/rest/resources/ModeResource.java`

5. **Multi-Datacenter:**
   - `core/src/main/java/io/confluent/kafka/schemaregistry/leaderelector/kafka/KafkaGroupLeaderElector.java`
   - `client/src/main/java/io/confluent/kafka/schemaregistry/utils/QualifiedSubject.java`

---

## Example: Custom Schema ID Registration

### Step 1: Enable IMPORT Mode

```bash
# Set mode.mutability=true in schema-registry.properties
# Restart Schema Registry

# Set IMPORT mode
curl -X PUT \
  -H "Content-Type: application/json" \
  http://localhost:8081/mode \
  --data '{"mode": "IMPORT"}'
```

### Step 2: Register Schema with Custom ID

```bash
curl -X POST \
  -H "Content-Type: application/json" \
  http://localhost:8081/subjects/my-topic-value/versions \
  --data '{
    "schema": "{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"id\",\"type\":\"int\"}]}",
    "schemaType": "AVRO",
    "id": 10000
  }'
```

**Response:**
```json
{
  "id": 10000
}
```

### Step 3: Verify Registration

```bash
# Get schema by ID
curl http://localhost:8081/schemas/ids/10000

# Get subjects for schema ID
curl http://localhost:8081/schemas/ids/10000/subjects
```

---

## Conclusion

**Recommendation:** ✅ **Proceed with Global Schema ID Management**

This research confirms that Confluent Schema Registry's IMPORT mode provides native support for externally managed Schema IDs. By implementing a Global Schema ID Coordinator and leveraging IMPORT mode, we can:

1. ✅ Eliminate schema ID clashes across regions
2. ✅ Selectively deploy schemas to relevant regions
3. ✅ Overcome the 20,000 schema limit per registry
4. ✅ Integrate with CI/CD for declarative schema management
5. ✅ Maintain gaps in Schema IDs without issues

**Next Steps:**
1. Review and approve this proposal
2. Design Global Schema ID Coordinator API
3. Start Phase 1: Proof of Concept
4. Validate with small-scale pilot

---

## References

### Documentation Sources
- Confluent Schema Registry API Reference
- Schema Registry Multi-Datacenter Deployment Guide
- Schema Registry Migration Guide
- Schema Linking Documentation

### Web Search Results
- "Use Schema Registry to Migrate Schemas in Confluent Platform"
- "Schema Registry Deployment Architectures for Confluent Platform"
- Stack Overflow: "How does Confluent's Schema Registry assign schema id's?"

### Internal Code Analysis
- Full codebase analysis of `schema-registry` repository
- Exploration of ID generation, validation, and mode management
- REST API endpoint analysis and request/response models

---

**Document Version:** 1.0
**Last Updated:** 2025-11-13
**Review Status:** Pending
