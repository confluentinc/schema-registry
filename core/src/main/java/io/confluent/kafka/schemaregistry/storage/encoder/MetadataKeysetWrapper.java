// Generated by the protocol buffer compiler.  DO NOT EDIT!
// NO CHECKED-IN PROTOBUF GENCODE
// source: metadata/v1/metadata.proto
// Protobuf Java Version: 4.29.3
package io.confluent.kafka.schemaregistry.storage.encoder;

/**
 * Protobuf type {@code metadata.v1.MetadataKeysetWrapper}
 */
@io.confluent.resourcemanager.api.annotations.Group("metadata")
@io.confluent.resourcemanager.api.annotations.Version("v1")
@com.fasterxml.jackson.annotation.JsonPropertyOrder({ "kind", "apiVersion" })
public final class MetadataKeysetWrapper extends com.google.protobuf.GeneratedMessage implements // @@protoc_insertion_point(message_implements:metadata.v1.MetadataKeysetWrapper)
        MetadataKeysetWrapperOrBuilder, io.confluent.resourcemanager.api.model.HasMetadata {

  private static final long serialVersionUID = 0L;

  static {
    com.google.protobuf.RuntimeVersion.validateProtobufGencodeVersion(com.google.protobuf.RuntimeVersion.RuntimeDomain.PUBLIC, /* major= */
            4, /* minor= */
            29, /* patch= */
            3, /* suffix= */
            "", MetadataKeysetWrapper.class.getName());
  }

  // Use MetadataKeysetWrapper.newBuilder() to construct.
  private MetadataKeysetWrapper(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
    super(builder);
  }

  private MetadataKeysetWrapper() {
  }

  public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() {
    return io.confluent.kafka.schemaregistry.storage.encoder.Metadata.internal_static_metadata_v1_MetadataKeysetWrapper_descriptor;
  }

  @java.lang.Override
  protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable() {
    return io.confluent.kafka.schemaregistry.storage.encoder.Metadata.internal_static_metadata_v1_MetadataKeysetWrapper_fieldAccessorTable.ensureFieldAccessorsInitialized(io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper.class, io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper.Builder.class);
  }

  private int bitField0_;

  public static final int METADATA_FIELD_NUMBER = 1;

  private io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta metadata_;

  /**
   * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
   * @return Whether the metadata field is set.
   */
  @java.lang.Override
  public boolean hasMetadata() {
    return ((bitField0_ & 0x00000001) != 0);
  }

  /**
   * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
   * @return The metadata.
   */
  @java.lang.Override
  public io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta getMetadata() {
    return metadata_ == null ? io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta.getDefaultInstance() : metadata_;
  }

  /**
   * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
   */
  @java.lang.Override
  public io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMetaOrBuilder getMetadataOrBuilder() {
    return metadata_ == null ? io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta.getDefaultInstance() : metadata_;
  }

  public static final int SPEC_FIELD_NUMBER = 2;

  private io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec spec_;

  /**
   * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
   * @return Whether the spec field is set.
   */
  @java.lang.Override
  public boolean hasSpec() {
    return ((bitField0_ & 0x00000002) != 0);
  }

  /**
   * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
   * @return The spec.
   */
  @java.lang.Override
  public io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec getSpec() {
    return spec_ == null ? io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec.getDefaultInstance() : spec_;
  }

  /**
   * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
   */
  @java.lang.Override
  public io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpecOrBuilder getSpecOrBuilder() {
    return spec_ == null ? io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec.getDefaultInstance() : spec_;
  }

  private byte memoizedIsInitialized = -1;

  @java.lang.Override
  public final boolean isInitialized() {
    byte isInitialized = memoizedIsInitialized;
    if (isInitialized == 1)
      return true;
    if (isInitialized == 0)
      return false;
    memoizedIsInitialized = 1;
    return true;
  }

  @java.lang.Override
  public void writeTo(com.google.protobuf.CodedOutputStream output) throws java.io.IOException {
    if (((bitField0_ & 0x00000001) != 0)) {
      output.writeMessage(1, getMetadata());
    }
    if (((bitField0_ & 0x00000002) != 0)) {
      output.writeMessage(2, getSpec());
    }
    getUnknownFields().writeTo(output);
  }

  @java.lang.Override
  public int getSerializedSize() {
    int size = memoizedSize;
    if (size != -1)
      return size;
    size = 0;
    if (((bitField0_ & 0x00000001) != 0)) {
      size += com.google.protobuf.CodedOutputStream.computeMessageSize(1, getMetadata());
    }
    if (((bitField0_ & 0x00000002) != 0)) {
      size += com.google.protobuf.CodedOutputStream.computeMessageSize(2, getSpec());
    }
    size += getUnknownFields().getSerializedSize();
    memoizedSize = size;
    return size;
  }

  @java.lang.Override
  public boolean equals(final java.lang.Object obj) {
    if (obj == this) {
      return true;
    }
    if (!(obj instanceof io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper)) {
      return super.equals(obj);
    }
    io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper other = (io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper) obj;
    if (hasMetadata() != other.hasMetadata())
      return false;
    if (hasMetadata()) {
      if (!getMetadata().equals(other.getMetadata()))
        return false;
    }
    if (hasSpec() != other.hasSpec())
      return false;
    if (hasSpec()) {
      if (!getSpec().equals(other.getSpec()))
        return false;
    }
    if (!getUnknownFields().equals(other.getUnknownFields()))
      return false;
    return true;
  }

  @java.lang.Override
  public int hashCode() {
    if (memoizedHashCode != 0) {
      return memoizedHashCode;
    }
    int hash = 41;
    hash = (19 * hash) + getDescriptor().hashCode();
    if (hasMetadata()) {
      hash = (37 * hash) + METADATA_FIELD_NUMBER;
      hash = (53 * hash) + getMetadata().hashCode();
    }
    if (hasSpec()) {
      hash = (37 * hash) + SPEC_FIELD_NUMBER;
      hash = (53 * hash) + getSpec().hashCode();
    }
    hash = (29 * hash) + getUnknownFields().hashCode();
    memoizedHashCode = hash;
    return hash;
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseFrom(java.nio.ByteBuffer data) throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseFrom(java.nio.ByteBuffer data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseFrom(com.google.protobuf.ByteString data) throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseFrom(com.google.protobuf.ByteString data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseFrom(byte[] data) throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseFrom(byte[] data, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseFrom(java.io.InputStream input) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage.parseWithIOException(PARSER, input);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage.parseWithIOException(PARSER, input, extensionRegistry);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseDelimitedFrom(java.io.InputStream input) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage.parseDelimitedWithIOException(PARSER, input);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseDelimitedFrom(java.io.InputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage.parseDelimitedWithIOException(PARSER, input, extensionRegistry);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseFrom(com.google.protobuf.CodedInputStream input) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage.parseWithIOException(PARSER, input);
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper parseFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException {
    return com.google.protobuf.GeneratedMessage.parseWithIOException(PARSER, input, extensionRegistry);
  }

  @java.lang.Override
  public Builder newBuilderForType() {
    return newBuilder();
  }

  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }

  public static Builder newBuilder(io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }

  @java.lang.Override
  public Builder toBuilder() {
    return this == DEFAULT_INSTANCE ? new Builder() : new Builder().mergeFrom(this);
  }

  @java.lang.Override
  protected Builder newBuilderForType(com.google.protobuf.GeneratedMessage.BuilderParent parent) {
    Builder builder = new Builder(parent);
    return builder;
  }

  /**
   * Protobuf type {@code metadata.v1.MetadataKeysetWrapper}
   */
  public static final class Builder extends com.google.protobuf.GeneratedMessage.Builder<Builder> implements // @@protoc_insertion_point(builder_implements:metadata.v1.MetadataKeysetWrapper)
          io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperOrBuilder {

    public static final com.google.protobuf.Descriptors.Descriptor getDescriptor() {
      return io.confluent.kafka.schemaregistry.storage.encoder.Metadata.internal_static_metadata_v1_MetadataKeysetWrapper_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable internalGetFieldAccessorTable() {
      return io.confluent.kafka.schemaregistry.storage.encoder.Metadata.internal_static_metadata_v1_MetadataKeysetWrapper_fieldAccessorTable.ensureFieldAccessorsInitialized(io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper.class, io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper.Builder.class);
    }

    // Construct using io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper.newBuilder()
    private Builder() {
      maybeForceBuilderInitialization();
    }

    private Builder(com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      super(parent);
      maybeForceBuilderInitialization();
    }

    private void maybeForceBuilderInitialization() {
      if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        getMetadataFieldBuilder();
        getSpecFieldBuilder();
      }
    }

    @java.lang.Override
    public Builder clear() {
      super.clear();
      bitField0_ = 0;
      metadata_ = null;
      if (metadataBuilder_ != null) {
        metadataBuilder_.dispose();
        metadataBuilder_ = null;
      }
      spec_ = null;
      if (specBuilder_ != null) {
        specBuilder_.dispose();
        specBuilder_ = null;
      }
      return this;
    }

    @java.lang.Override
    public com.google.protobuf.Descriptors.Descriptor getDescriptorForType() {
      return io.confluent.kafka.schemaregistry.storage.encoder.Metadata.internal_static_metadata_v1_MetadataKeysetWrapper_descriptor;
    }

    @java.lang.Override
    public io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper getDefaultInstanceForType() {
      return io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper.getDefaultInstance();
    }

    @java.lang.Override
    public io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper build() {
      io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper result = buildPartial();
      if (!result.isInitialized()) {
        throw newUninitializedMessageException(result);
      }
      return result;
    }

    @java.lang.Override
    public io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper buildPartial() {
      io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper result = new io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper(this);
      if (bitField0_ != 0) {
        buildPartial0(result);
      }
      onBuilt();
      return result;
    }

    private void buildPartial0(io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper result) {
      int from_bitField0_ = bitField0_;
      int to_bitField0_ = 0;
      if (((from_bitField0_ & 0x00000001) != 0)) {
        result.metadata_ = metadataBuilder_ == null ? metadata_ : metadataBuilder_.build();
        to_bitField0_ |= 0x00000001;
      }
      if (((from_bitField0_ & 0x00000002) != 0)) {
        result.spec_ = specBuilder_ == null ? spec_ : specBuilder_.build();
        to_bitField0_ |= 0x00000002;
      }
      result.bitField0_ |= to_bitField0_;
    }

    @java.lang.Override
    public Builder mergeFrom(com.google.protobuf.Message other) {
      if (other instanceof io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper) {
        return mergeFrom((io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper) other);
      } else {
        super.mergeFrom(other);
        return this;
      }
    }

    public Builder mergeFrom(io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper other) {
      if (other == io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper.getDefaultInstance())
        return this;
      if (other.hasMetadata()) {
        mergeMetadata(other.getMetadata());
      }
      if (other.hasSpec()) {
        mergeSpec(other.getSpec());
      }
      this.mergeUnknownFields(other.getUnknownFields());
      onChanged();
      return this;
    }

    @java.lang.Override
    public final boolean isInitialized() {
      return true;
    }

    @java.lang.Override
    public Builder mergeFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws java.io.IOException {
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch(tag) {
            case 0:
              done = true;
              break;
            case 10:
            {
              input.readMessage(getMetadataFieldBuilder().getBuilder(), extensionRegistry);
              bitField0_ |= 0x00000001;
              break;
            }
            // case 10
            case 18:
            {
              input.readMessage(getSpecFieldBuilder().getBuilder(), extensionRegistry);
              bitField0_ |= 0x00000002;
              break;
            }
            // case 18
            default:
            {
              if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                // was an endgroup tag
                done = true;
              }
              break;
            }
          }
          // switch (tag)
        }
        // while (!done)
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.unwrapIOException();
      } finally {
        onChanged();
      }
      // finally
      return this;
    }

    private int bitField0_;

    private io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta metadata_;

    private com.google.protobuf.SingleFieldBuilder<io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta, io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta.Builder, io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMetaOrBuilder> metadataBuilder_;

    /**
     * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
     * @return Whether the metadata field is set.
     */
    public boolean hasMetadata() {
      return ((bitField0_ & 0x00000001) != 0);
    }

    /**
     * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
     * @return The metadata.
     */
    public io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta getMetadata() {
      if (metadataBuilder_ == null) {
        return metadata_ == null ? io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta.getDefaultInstance() : metadata_;
      } else {
        return metadataBuilder_.getMessage();
      }
    }

    /**
     * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
     */
    public Builder setMetadata(io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta value) {
      if (metadataBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        metadata_ = value;
      } else {
        metadataBuilder_.setMessage(value);
      }
      bitField0_ |= 0x00000001;
      onChanged();
      return this;
    }

    /**
     * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
     */
    public Builder setMetadata(io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta.Builder builderForValue) {
      if (metadataBuilder_ == null) {
        metadata_ = builderForValue.build();
      } else {
        metadataBuilder_.setMessage(builderForValue.build());
      }
      bitField0_ |= 0x00000001;
      onChanged();
      return this;
    }

    /**
     * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
     */
    public Builder mergeMetadata(io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta value) {
      if (metadataBuilder_ == null) {
        if (((bitField0_ & 0x00000001) != 0) && metadata_ != null && metadata_ != io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta.getDefaultInstance()) {
          getMetadataBuilder().mergeFrom(value);
        } else {
          metadata_ = value;
        }
      } else {
        metadataBuilder_.mergeFrom(value);
      }
      if (metadata_ != null) {
        bitField0_ |= 0x00000001;
        onChanged();
      }
      return this;
    }

    /**
     * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
     */
    public Builder clearMetadata() {
      bitField0_ = (bitField0_ & ~0x00000001);
      metadata_ = null;
      if (metadataBuilder_ != null) {
        metadataBuilder_.dispose();
        metadataBuilder_ = null;
      }
      onChanged();
      return this;
    }

    /**
     * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
     */
    public io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta.Builder getMetadataBuilder() {
      bitField0_ |= 0x00000001;
      onChanged();
      return getMetadataFieldBuilder().getBuilder();
    }

    /**
     * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
     */
    public io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMetaOrBuilder getMetadataOrBuilder() {
      if (metadataBuilder_ != null) {
        return metadataBuilder_.getMessageOrBuilder();
      } else {
        return metadata_ == null ? io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta.getDefaultInstance() : metadata_;
      }
    }

    /**
     * <code>.resourcemanager.meta.v1.ObjectMeta metadata = 1;</code>
     */
    private com.google.protobuf.SingleFieldBuilder<io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta, io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta.Builder, io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMetaOrBuilder> getMetadataFieldBuilder() {
      if (metadataBuilder_ == null) {
        metadataBuilder_ = new com.google.protobuf.SingleFieldBuilder<io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta, io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMeta.Builder, io.confluent.resourcemanager.protobuf.apis.meta.v1.ObjectMetaOrBuilder>(getMetadata(), getParentForChildren(), isClean());
        metadata_ = null;
      }
      return metadataBuilder_;
    }

    private io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec spec_;

    private com.google.protobuf.SingleFieldBuilder<io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec, io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec.Builder, io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpecOrBuilder> specBuilder_;

    /**
     * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
     * @return Whether the spec field is set.
     */
    public boolean hasSpec() {
      return ((bitField0_ & 0x00000002) != 0);
    }

    /**
     * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
     * @return The spec.
     */
    public io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec getSpec() {
      if (specBuilder_ == null) {
        return spec_ == null ? io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec.getDefaultInstance() : spec_;
      } else {
        return specBuilder_.getMessage();
      }
    }

    /**
     * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
     */
    public Builder setSpec(io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec value) {
      if (specBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        spec_ = value;
      } else {
        specBuilder_.setMessage(value);
      }
      bitField0_ |= 0x00000002;
      onChanged();
      return this;
    }

    /**
     * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
     */
    public Builder setSpec(io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec.Builder builderForValue) {
      if (specBuilder_ == null) {
        spec_ = builderForValue.build();
      } else {
        specBuilder_.setMessage(builderForValue.build());
      }
      bitField0_ |= 0x00000002;
      onChanged();
      return this;
    }

    /**
     * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
     */
    public Builder mergeSpec(io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec value) {
      if (specBuilder_ == null) {
        if (((bitField0_ & 0x00000002) != 0) && spec_ != null && spec_ != io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec.getDefaultInstance()) {
          getSpecBuilder().mergeFrom(value);
        } else {
          spec_ = value;
        }
      } else {
        specBuilder_.mergeFrom(value);
      }
      if (spec_ != null) {
        bitField0_ |= 0x00000002;
        onChanged();
      }
      return this;
    }

    /**
     * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
     */
    public Builder clearSpec() {
      bitField0_ = (bitField0_ & ~0x00000002);
      spec_ = null;
      if (specBuilder_ != null) {
        specBuilder_.dispose();
        specBuilder_ = null;
      }
      onChanged();
      return this;
    }

    /**
     * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
     */
    public io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec.Builder getSpecBuilder() {
      bitField0_ |= 0x00000002;
      onChanged();
      return getSpecFieldBuilder().getBuilder();
    }

    /**
     * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
     */
    public io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpecOrBuilder getSpecOrBuilder() {
      if (specBuilder_ != null) {
        return specBuilder_.getMessageOrBuilder();
      } else {
        return spec_ == null ? io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec.getDefaultInstance() : spec_;
      }
    }

    /**
     * <code>.metadata.v1.MetadataKeysetWrapperSpec spec = 2;</code>
     */
    private com.google.protobuf.SingleFieldBuilder<io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec, io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec.Builder, io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpecOrBuilder> getSpecFieldBuilder() {
      if (specBuilder_ == null) {
        specBuilder_ = new com.google.protobuf.SingleFieldBuilder<io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec, io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpec.Builder, io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapperSpecOrBuilder>(getSpec(), getParentForChildren(), isClean());
        spec_ = null;
      }
      return specBuilder_;
    }
    // @@protoc_insertion_point(builder_scope:metadata.v1.MetadataKeysetWrapper)
  }

  // @@protoc_insertion_point(class_scope:metadata.v1.MetadataKeysetWrapper)
  private static final io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper DEFAULT_INSTANCE;

  static {
    DEFAULT_INSTANCE = new io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper();
  }

  public static io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static final com.google.protobuf.Parser<MetadataKeysetWrapper> PARSER = new com.google.protobuf.AbstractParser<MetadataKeysetWrapper>() {

    @java.lang.Override
    public MetadataKeysetWrapper parsePartialFrom(com.google.protobuf.CodedInputStream input, com.google.protobuf.ExtensionRegistryLite extensionRegistry) throws com.google.protobuf.InvalidProtocolBufferException {
      Builder builder = newBuilder();
      try {
        builder.mergeFrom(input, extensionRegistry);
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(builder.buildPartial());
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(e).setUnfinishedMessage(builder.buildPartial());
      }
      return builder.buildPartial();
    }
  };

  public static com.google.protobuf.Parser<MetadataKeysetWrapper> parser() {
    return PARSER;
  }

  @java.lang.Override
  public com.google.protobuf.Parser<MetadataKeysetWrapper> getParserForType() {
    return PARSER;
  }

  @java.lang.Override
  public io.confluent.kafka.schemaregistry.storage.encoder.MetadataKeysetWrapper getDefaultInstanceForType() {
    return DEFAULT_INSTANCE;
  }

  public io.confluent.resourcemanager.api.model.identifier.ObjectIdentifier<MetadataKeysetWrapper> identifier() {
    return new io.confluent.resourcemanager.api.model.identifier.ObjectIdentifier<>(io.confluent.resourcemanager.api.model.location.Location.from(getMetadata().getLocation()), getMetadata().getScopeList(), getMetadata().getName(), this.getClass());
  }
}
